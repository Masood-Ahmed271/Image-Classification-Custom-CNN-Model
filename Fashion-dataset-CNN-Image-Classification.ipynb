{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRdgPVhv9kd"
      },
      "source": [
        "# Deep Learning - Image Classification - CNN - Fashion Image Dataset\n",
        "\n",
        "### Author: Masood Ahmed\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pH4GZlKQwNI1"
      },
      "source": [
        "## Part 1: Image Classification using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6mjHKojwRul"
      },
      "source": [
        "A zip file containing the dataset has been put in the google drive and then google drive is mounted. We are using this method becuase it is much more efficient to load data when using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r4ym1_9JZQ4"
      },
      "outputs": [],
      "source": [
        "# mounting google drive inorder to upload the dataset (will take about 1 minute for a dataset as large as 1GB - Atleast for me).\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbops8gVwlen"
      },
      "source": [
        "Using the OS library, we are just checking what directory structure we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkJ8cIYrJrX-"
      },
      "outputs": [],
      "source": [
        "# To check the directories we have\n",
        "import os\n",
        "\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vAB7BqsJfWv"
      },
      "outputs": [],
      "source": [
        "# uploading data from google drive and /dev/null is used  to suppress the output\n",
        "!unzip gdrive/My\\ Drive/Advanced-DataAnalytics-Assignment2/Fashion-Product-Images.zip > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llVWyIalwwSj"
      },
      "source": [
        "We have also placed the test.csv and train.cvs in the google drive and we are using the locations of those files in the google colab to access those files and load it into the dataframes for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7XUDQcLKBLo"
      },
      "outputs": [],
      "source": [
        "# importing the train csv file from the google drive\n",
        "import pandas as pd\n",
        "\n",
        "train_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/train.csv'\n",
        "\n",
        "train_df=pd.read_csv(train_path, sep='\\t')\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrF1hpStLNLH"
      },
      "outputs": [],
      "source": [
        "# importing the test csv file from the google drive\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "test_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/test.csv'\n",
        "\n",
        "test_df=pd.read_csv(test_path, sep='\\t')\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RII2mg4RxKPl"
      },
      "source": [
        "Picking random picture from the dataset and checking it's dimensions and color channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0qK7Wh5GLRit"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# setting the seed for reproducibilty\n",
        "myseed = 12345\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# let's take a look at one random image\n",
        "\n",
        "random_pic_file = random.choice(os.listdir('Fashion-Product-Images/images/'))\n",
        "pic = imageio.imread('Fashion-Product-Images/images/' + random_pic_file)\n",
        "plt.imshow(pic)\n",
        "height, width, channels = pic.shape\n",
        "print(f'original height, width, and channels of each image: {height} {width} {channels}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKQUZ-ETxv6u"
      },
      "source": [
        "We are now adding an additional column which contains path to the images relative to the folder containing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4P_TvJfLf-N"
      },
      "outputs": [],
      "source": [
        "# creating an other column in the dataframe for image paths\n",
        "train_df['image-path'] = train_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "test_df['image-path'] = test_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt0dkC4dyC3u"
      },
      "source": [
        "Changing the datatype of the imageid which will help us in the processing of teh data and taining of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDZNVug7Liyk"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.astype({'imageid':'string'})\n",
        "print(train_df.dtypes)\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_bS6LUTyUmc"
      },
      "source": [
        "Doing encoding of labels from strings/objects to numerics for training purpose. If not done, the training model will crash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2bMRFcTL1xF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "cat_cols = ['label']\n",
        "\n",
        "# apply label encoder to categorical columns\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(train_df['label'])\n",
        "train_df[cat_cols] = train_df[cat_cols].apply(lambda x: le.fit_transform(x))\n",
        "\n",
        "# show the transformed dataframe\n",
        "print(train_df.head())\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "train_df['label'].unique()\n",
        "unique_labels = np.unique(labels)\n",
        "print(unique_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41aeqTcoRG-"
      },
      "source": [
        "Doing Train-Validation Split for 0.3 test size and shuffle true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjEgPrfUoY2s"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, shuffle = True, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ5-TvZNyogp"
      },
      "source": [
        "Doing the same preprocessing on the test.csv as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRs5IEudL_pk"
      },
      "outputs": [],
      "source": [
        "test_df = test_df.astype({'imageid':'string'})\n",
        "print(test_df.dtypes)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JZ9vqPhMZp0"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages.\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "from torchvision.transforms import Resize\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# This is for the progress bar.\n",
        "from tqdm.auto import tqdm\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHPxURz020Rv"
      },
      "source": [
        "Here we are doing a basic setup of the pytorch for our CNN model.\n",
        "\n",
        "**torch.backends.cudnn.deterministic = True:** This line ensures that the CuDNN backend uses deterministic algorithms for convolution operations. This can be important for ensuring reproducibility when training on GPUs, as the non-deterministic nature of GPU operations can lead to slightly different results each time the code is run.\n",
        "\n",
        "**torch.backends.cudnn.benchmark = False:** This line disables the CuDNN benchmark mode, which is used to automatically find the best algorithms for convolution operations based on the input size and shape. While this can improve performance, it can also lead to slightly different results each time the code is run.\n",
        "\n",
        "**np.random.seed(myseed):** This sets the seed for the NumPy random number generator, which is used for some random operations in the code.\n",
        "\n",
        "**torch.manual_seed(myseed):** This sets the seed for the PyTorch random number generator, which is used for other random operations in the code.\n",
        "\n",
        "**if torch.cuda.is_available(): torch.cuda.manual_seed_all(myseed):** This sets the seed for the PyTorch CUDA random number generator, which is used for random operations when running the code on a GPU. If a GPU is not available, this line is skipped. By setting the same seed for all three random number generators, the code will produce the same results each time it is run, assuming that all other factors (such as the input data and model architecture) remain the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niRuFZ7pMdOt"
      },
      "outputs": [],
      "source": [
        "# basic setup for PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBGtGi2z2QOI"
      },
      "source": [
        "transform.compose function is used to do data-processing for example resizing the images, making sure each image have same color channels and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guyqed0iMgkd"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "\n",
        "val_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siSFB9TK0AmU"
      },
      "source": [
        "### Custom Dataset and Datalaoder\n",
        "\n",
        "We are creating a custom dataset class which is inherited from python's class called dataset. We have to do this because we have a different structure of images and labels therefore we need to adjust the class according to our needs. \n",
        "\n",
        "In init function, we load the csv files with labels and the image directory as well. We have also added a transform variable to support any transform that we do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cq4n7uyMjs9"
      },
      "outputs": [],
      "source": [
        "image_path = 'Fashion-Product-Images/images/'\n",
        "\n",
        "class CustomImageDataset_from_csv(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        print(img_path)\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EfC20sir8Q0"
      },
      "outputs": [],
      "source": [
        "image_path = 'Fashion-Product-Images/images/'\n",
        "\n",
        "class CustomImageDataset_from_csv_test(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label, img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySHfwib49Q-"
      },
      "source": [
        "# Convolutional Neural Network Class\n",
        "\n",
        "### Simple Explanation: \n",
        "\n",
        "We are creating our CNN class which is inherited from nn.Module class of python. In the init class, we specify all our layers. Here we have applied the Conv2d filter, batch normalization, relu layer to bring the non-linearity, then we add the maxpooling layer with a kernel size of 2 which will reduce the width and height of the convolutional output by a factor of 2. Then we add multiple layers adjusting the input_channels and output_channels.\n",
        "\n",
        "Lastly we add the fully connected layer where we feed the number of input feature. Then we define the forward function where we pass the informtion from the above layers.\n",
        "\n",
        "### Motivation: \n",
        "\n",
        "The motivation behind this CNN model is to perform image classification on a dataset with 13 different classes. The model consists of several convolutional layers with batch normalization, ReLU activation, and max-pooling operations, followed by fully connected layers to classify the input images.\n",
        "\n",
        "The first layer of the model is a convolutional layer with 64 filters and a 3x3 kernel size, which takes the input image with 3 channels and outputs feature maps of size 64x128x128. The output feature maps are then normalized using batch normalization, and the ReLU activation function is applied to introduce non-linearity. Then, a max-pooling layer with a 2x2 kernel size is used to downsample the feature maps, reducing the spatial dimensions by half and producing feature maps of size 64x64x64.\n",
        "\n",
        "The same process is repeated with the second, third, fourth, and fifth convolutional layers, with increasing numbers of filters and decreasing spatial dimensions. The final convolutional layer produces feature maps of size 512x4x4.\n",
        "\n",
        "After the convolutional layers, the output feature maps are flattened and fed into a fully connected neural network consisting of three linear layers with ReLU activation functions. The final output layer has 13 units, corresponding to the 13 different classes in the dataset.\n",
        "\n",
        "Overall, the motivation behind this CNN model is to leverage the power of convolutional layers to extract meaningful features from the input images and to use fully connected layers to classify these features into different classes. The use of batch normalization and ReLU activation helps improve the stability and performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yekW-4FTMqQl"
      },
      "outputs": [],
      "source": [
        "class FirstCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstCNN, self).__init__()\n",
        "       \n",
        "        # input size [3, 128, 128]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "            \n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 13)\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDlLBvpq3eOU"
      },
      "source": [
        "### Dataloaders\n",
        "\n",
        "In pytorch we feed the data in the form of dataloaders in batches for training. As we cannot feed all the images as once, as it will cause a memory overload, and as it is not an optimize way in deep learning, therefore we will use batches. In the datalaoder, we have made shuffle to true to ensure that our model is not biased to any categories. Our batch_size (hyperparameter) is 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD9DKe21M2_k"
      },
      "outputs": [],
      "source": [
        "_exp_name = \"sample_1\"\n",
        "batch_size = 64\n",
        "\n",
        "train_data = CustomImageDataset_from_csv(train_df , image_path , transform = train_tfm)\n",
        "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "val_data = CustomImageDataset_from_csv(val_df , image_path , transform = val_tfm)\n",
        "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "test_data = CustomImageDataset_from_csv_test(test_df , image_path , transform = test_tfm)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = False, num_workers=0, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jynwV4Sp9Se5"
      },
      "outputs": [],
      "source": [
        "# Selecting which device to use for training. It would be better to use cuda if GPU is available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG9CuhO8JShC"
      },
      "outputs": [],
      "source": [
        "# Initializing the model, and put it on the device specified.\n",
        "model = FirstCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ZJlAWA9l3P"
      },
      "source": [
        "Setting up the optimizer, we are using adam optimizer and for the loss we are using cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Rn5WNR9lLJ"
      },
      "outputs": [],
      "source": [
        "# Initialize optimizer!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5) \n",
        "\n",
        "# For the classification task, we use cross-entropy as the measurement of performance.\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfkjnkHS-aZy"
      },
      "source": [
        "Here we are running the model and we will save the model for the epoch which will provide us the best training accuracy!\n",
        "\n",
        "### Brief Description:\n",
        "\n",
        "This code trains a convolutional neural network (CNN) using PyTorch for image classification. It first sets the number of training epochs and patience (the number of epochs with no improvement before stopping early). It then initializes the CNN model and puts it on the specified device (e.g., CPU or GPU).\n",
        "\n",
        "The code then iterates through each epoch and trains the model on the training dataset. For each batch of images and labels, the model computes the forward pass to generate logits, calculates the cross-entropy loss, computes the gradients, clips the gradient norms for stable training, updates the parameters with computed gradients, and records the loss and accuracy. The model then evaluates on the validation dataset by iterating through each batch and recording the loss and accuracy. If the validation accuracy improves, the best model is saved, and the early stop counter is reset. If there is no improvement in the validation accuracy for patience consecutive epochs, the training is stopped early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFEbnxqAM6mT"
      },
      "outputs": [],
      "source": [
        "# The number of training epochs (hyperparameters) and patience.\n",
        "n_epochs = 4\n",
        "patience = 300 # If no improvement in 'patience' epochs, early stop\n",
        "\n",
        "\n",
        "# Initializing trackers\n",
        "stale = 0\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    ## Evaluation and training on training dataset.\n",
        "\n",
        "    model.train()        # model is in train mode before training.\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.to(device)  # move images to device\n",
        "        labels = torch.tensor(labels).to(device)  # move labels to device\n",
        "\n",
        "        # Forward the data\n",
        "        logits = model(imgs)\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the gradients for parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "        \n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    ## Evaluation and testing on validation dataset\n",
        "\n",
        "    model.eval()       # model is in eval mode\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # Iterate the validation set by batches.\n",
        "    for batch in tqdm(val_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "        #break\n",
        "\n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc:\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_vtKeHrlqOg1"
      },
      "source": [
        "#### making a csv file with prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwYTXsZoOwZF"
      },
      "outputs": [],
      "source": [
        "model_best = FirstCNN().to(device)\n",
        "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
        "model_best.eval()\n",
        "\n",
        "prediction = []\n",
        "image_paths = []\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data,_, img in test_dataloader:\n",
        "        test_pred = model_best(data.to(device))\n",
        "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
        "        prediction += test_label.squeeze().tolist()\n",
        "        image_paths += list(img)\n",
        "\n",
        "\n",
        "print(prediction)\n",
        "print(image_paths)\n",
        "mapping = {0: \"Bags\", 1: \"Bottomwear\", 2: \"Eyewear\", 3: \"Fragrance\", 4: \"Innerwear\", 5: \"Jewellery\", 6: \"Makeup\", 7: \"Others\", 8: \"Sandal\", 9: \"Shoes\", 10: \"Topwear\", 11: \"Wallets\", 12: \"watches\" }\n",
        "\n",
        "# now creating a prediction csv\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(prediction)):\n",
        "  # append rows to an empty DataFrame\n",
        "  df = df.append({'image-path' : image_paths[i], 'encoded-label' : prediction[i], 'label-decoded' : mapping[prediction[i]]},\n",
        "        ignore_index = True)\n",
        "\n",
        "df.to_csv(\"prediction_1.csv\",index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baJdTDFtBmyi"
      },
      "source": [
        "## Conclusion:\n",
        "\n",
        "Our model provides us with an accuracy of 95 percent which was found at 3rd epoch therefore we saved that model. The training time was quite high due to large number of images. It was as high as 8-10 hours (depending on what GPU/CPU are you using). Overall, the accuracy given by the model is pretty high, especially for the given dataset related to fashion images as it was aimimg at distinguishing between very similar fashion items such as shoes and sandals and innerwear and outerwear and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk-N8iObuajc"
      },
      "source": [
        "# Part 2: Improved Image Classification\n",
        "\n",
        "### Tuning one hyper-parameter and explain why this is worth to tune."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuJazrJRywXg"
      },
      "source": [
        "## Hyperparameter Choosen:\n",
        "\n",
        "I am choosing the batch size hyperparameter to tune my model. Given that the current experiment takes 8 hours to train and has a batch size of 64 and the experiment with batch size of 64 has an accuracy of 95%, I believe that increasing batch size would help us reduce the training time a lot.\n",
        "\n",
        "When adjusting the batch size, I tried increasing it by a factor of 2 and then by 4 and observed the effect on model performance. By increase the batch size, I was able to speed up training time as more data could be processed in parallel. However, a larger batch size may also lead to a decrease in model performance as the gradient estimate becomes less accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSEAP110zstZ"
      },
      "source": [
        "##### Repeating the same training process as above with a batch size of 128 and 256 respectively!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xv-XETufUiK"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# setting the seed for reproducibilty\n",
        "myseed = 12345\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "from torchvision.transforms import Resize\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVJYCnEAzr5B"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/test.csv'\n",
        "\n",
        "test_df=pd.read_csv(test_path, sep='\\t')\n",
        "\n",
        "train_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/train.csv'\n",
        "\n",
        "train_df=pd.read_csv(train_path, sep='\\t')\n",
        "\n",
        "train_df['image-path'] = train_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "test_df['image-path'] = test_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "\n",
        "train_df = train_df.astype({'imageid':'string'})\n",
        "test_df = test_df.astype({'imageid':'string'})\n",
        "\n",
        "cat_cols = ['label']\n",
        "\n",
        "# apply label encoder to categorical columns\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(train_df['label'])\n",
        "train_df[cat_cols] = train_df[cat_cols].apply(lambda x: le.fit_transform(x))\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "train_df['label'].unique()\n",
        "unique_labels = np.unique(labels)\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, shuffle = True, test_size = 0.3)\n",
        "\n",
        "# basic setup for PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "  \n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "\n",
        "val_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "\n",
        "])\n",
        "\n",
        "image_path = 'Fashion-Product-Images/images/'\n",
        "\n",
        "class CustomImageDataset_from_csv(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        print(img_path)\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label)\n",
        "\n",
        "class CustomImageDataset_from_csv_test(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label, img_path)\n",
        "\n",
        "class FirstCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstCNN, self).__init__()\n",
        "       \n",
        "        # input size [3, 128, 128]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "            \n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 13)\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7rOziZc1ate"
      },
      "source": [
        "## Tuning batch size and putting it 128 now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADExWN-Q1Y1x"
      },
      "outputs": [],
      "source": [
        "_exp_name = \"sample_2\"\n",
        "batch_size = 128\n",
        "\n",
        "train_data = CustomImageDataset_from_csv(train_df , image_path , transform = train_tfm)\n",
        "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "val_data = CustomImageDataset_from_csv(val_df , image_path , transform = val_tfm)\n",
        "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "test_data = CustomImageDataset_from_csv_test(test_df , image_path , transform = test_tfm)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = False, num_workers=0, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4uesQe81iCw"
      },
      "outputs": [],
      "source": [
        "# Selecting which device to use for training. It would be better to use cuda if GPU is available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initializing the model, and put it on the device specified.\n",
        "model = FirstCNN().to(device)\n",
        "\n",
        "# Initialize optimizer!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5) \n",
        "\n",
        "# For the classification task, we use cross-entropy as the measurement of performance.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# The number of training epochs (hyperparameters) and patience.\n",
        "n_epochs = 4\n",
        "patience = 300 # If no improvement in 'patience' epochs, early stop\n",
        "\n",
        "\n",
        "# Initializing trackers\n",
        "stale = 0\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    ## Evaluation and training on training dataset.\n",
        "\n",
        "    model.train()        # model is in train mode before training.\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.to(device)  # move images to device\n",
        "        labels = torch.tensor(labels).to(device)  # move labels to device\n",
        "\n",
        "        # Forward the data\n",
        "        logits = model(imgs)\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the gradients for parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "        \n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    ## Evaluation and testing on validation dataset\n",
        "\n",
        "    model.eval()       # model is in eval mode\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # Iterate the validation set by batches.\n",
        "    for batch in tqdm(val_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "        #break\n",
        "\n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc:\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brx5kW5y1zuQ"
      },
      "outputs": [],
      "source": [
        "model_best = FirstCNN().to(device)\n",
        "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
        "model_best.eval()\n",
        "\n",
        "prediction = []\n",
        "image_paths = []\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data,_, img in test_dataloader:\n",
        "        test_pred = model_best(data.to(device))\n",
        "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
        "        prediction += test_label.squeeze().tolist()\n",
        "        image_paths += list(img)\n",
        "\n",
        "\n",
        "print(prediction)\n",
        "print(image_paths)\n",
        "mapping = {0: \"Bags\", 1: \"Bottomwear\", 2: \"Eyewear\", 3: \"Fragrance\", 4: \"Innerwear\", 5: \"Jewellery\", 6: \"Makeup\", 7: \"Others\", 8: \"Sandal\", 9: \"Shoes\", 10: \"Topwear\", 11: \"Wallets\", 12: \"watches\" }\n",
        "\n",
        "# now creating a prediction csv\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(prediction)):\n",
        "  # append rows to an empty DataFrame\n",
        "  df = df.append({'image-path' : image_paths[i], 'encoded-label' : prediction[i], 'label-decoded' : mapping[prediction[i]]},\n",
        "        ignore_index = True)\n",
        "\n",
        "df.to_csv(\"prediction_2.csv\",index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2k_9rOQjWHz"
      },
      "source": [
        "## Conclusion: \n",
        "\n",
        "In conclusion, the comparison between two CNN image classification models with different batch sizes (hyper-parameter) and training times showed that a longer training time of 8 hours with a smaller batch size of 64 resulted in a higher accuracy of 95% when compared to a shorter training time of 6 hours with a larger batch size of 128, which resulted in an accuracy of 93%. Although the dataset used contained 40000 data-points, it was not sufficient to offset the negative impact of the larger batch size on the model's accuracy. Therefore, choosing an appropriate batch size and training time is critical for optimizing the performance and accuracy of CNN image classification models, depending on the specific dataset and available computational resources. \n",
        "\n",
        "In my case, as I was working on my personal machine which was being used for other purposes as well, I believe a reduction in training time will benefit a lot given that there is not much reduction in accuracy of the model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A2nmXgvZ12sL"
      },
      "source": [
        "#### Now the following script is for increasing the batch-size from 128 to 256 and seeing how the results are!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrUeL08ZfM8k"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# setting the seed for reproducibilty\n",
        "myseed = 12345\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "from torchvision.transforms import Resize\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Me7nLe2HBJ"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/test.csv'\n",
        "\n",
        "test_df=pd.read_csv(test_path, sep='\\t')\n",
        "\n",
        "train_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/train.csv'\n",
        "\n",
        "train_df=pd.read_csv(train_path, sep='\\t')\n",
        "\n",
        "train_df['image-path'] = train_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "test_df['image-path'] = test_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "\n",
        "train_df = train_df.astype({'imageid':'string'})\n",
        "test_df = test_df.astype({'imageid':'string'})\n",
        "\n",
        "cat_cols = ['label']\n",
        "\n",
        "# apply label encoder to categorical columns\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(train_df['label'])\n",
        "train_df[cat_cols] = train_df[cat_cols].apply(lambda x: le.fit_transform(x))\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "train_df['label'].unique()\n",
        "unique_labels = np.unique(labels)\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, shuffle = True, test_size = 0.3)\n",
        "\n",
        "# basic setup for PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "  \n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "\n",
        "val_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "\n",
        "])\n",
        "\n",
        "image_path = 'Fashion-Product-Images/images/'\n",
        "\n",
        "class CustomImageDataset_from_csv(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        print(img_path)\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label)\n",
        "\n",
        "class CustomImageDataset_from_csv_test(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label, img_path)\n",
        "\n",
        "class FirstCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstCNN, self).__init__()\n",
        "       \n",
        "        # input size [3, 128, 128]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "            \n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 13)\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzDskQgG2KGS"
      },
      "outputs": [],
      "source": [
        "_exp_name = \"sample_3\"\n",
        "batch_size = 256\n",
        "\n",
        "train_data = CustomImageDataset_from_csv(train_df , image_path , transform = train_tfm)\n",
        "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "val_data = CustomImageDataset_from_csv(val_df , image_path , transform = val_tfm)\n",
        "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "test_data = CustomImageDataset_from_csv_test(test_df , image_path , transform = test_tfm)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = False, num_workers=0, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whBIS8uS2Uw1"
      },
      "outputs": [],
      "source": [
        "# Selecting which device to use for training. It would be better to use cuda if GPU is available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initializing the model, and put it on the device specified.\n",
        "model = FirstCNN().to(device)\n",
        "\n",
        "# Initialize optimizer!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5) \n",
        "\n",
        "# For the classification task, we use cross-entropy as the measurement of performance.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# The number of training epochs (hyperparameters) and patience.\n",
        "n_epochs = 4\n",
        "patience = 300 # If no improvement in 'patience' epochs, early stop\n",
        "\n",
        "\n",
        "# Initializing trackers\n",
        "stale = 0\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    ## Evaluation and training on training dataset.\n",
        "\n",
        "    model.train()        # model is in train mode before training.\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.to(device)  # move images to device\n",
        "        labels = torch.tensor(labels).to(device)  # move labels to device\n",
        "\n",
        "        # Forward the data\n",
        "        logits = model(imgs)\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the gradients for parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "        \n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    ## Evaluation and testing on validation dataset\n",
        "\n",
        "    model.eval()       # model is in eval mode\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # Iterate the validation set by batches.\n",
        "    for batch in tqdm(val_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "        #break\n",
        "\n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc:\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfe9v7RB2b4S"
      },
      "outputs": [],
      "source": [
        "model_best = FirstCNN().to(device)\n",
        "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
        "model_best.eval()\n",
        "\n",
        "prediction = []\n",
        "image_paths = []\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data,_, img in test_dataloader:\n",
        "        test_pred = model_best(data.to(device))\n",
        "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
        "        prediction += test_label.squeeze().tolist()\n",
        "        image_paths += list(img)\n",
        "\n",
        "\n",
        "mapping = {0: \"Bags\", 1: \"Bottomwear\", 2: \"Eyewear\", 3: \"Fragrance\", 4: \"Innerwear\", 5: \"Jewellery\", 6: \"Makeup\", 7: \"Others\", 8: \"Sandal\", 9: \"Shoes\", 10: \"Topwear\", 11: \"Wallets\", 12: \"watches\" }\n",
        "\n",
        "# now creating a prediction csv\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(prediction)):\n",
        "  # append rows to an empty DataFrame\n",
        "  df = df.append({'image-path' : image_paths[i], 'encoded-label' : prediction[i], 'label-decoded' : mapping[prediction[i]]},\n",
        "        ignore_index = True)\n",
        "\n",
        "df.to_csv(\"prediction_3.csv\",index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzmYtu5qkjQY"
      },
      "source": [
        "## Conclusion:\n",
        "\n",
        "We did an experiment again with increasing the batch size by 4 from 64 to 256. The prediction accuracy with this batch size was also close to 93% but training time had significantly reduced from 8-10 hours of training to approximately 4 hours of training. This reduction in training time plays an important role as it allows us to adjust hyperparameters further improving the model accuracy while not consuming to much machine time and power.\n",
        "\n",
        "In general, a larger batch size can lead to faster model development and better efficiency, but this must be balanced against the risk of overfitting and lower accuracy but in our case the drop in accuracy is not that much so we can consider keeping the batch size of 256 for further processing and training if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt4nYeMe2diS"
      },
      "source": [
        "## Data augmentation, i.e., generating more images for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbweOCxI62-T"
      },
      "source": [
        "We will generate new data-points from the original ones by rotating the images horizontally, and using grayscale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nXXkjGRfEgQ"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# setting the seed for reproducibilty\n",
        "myseed = 12345\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "from torchvision.transforms import Resize\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5I_PGmr8X9-"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/test.csv'\n",
        "\n",
        "test_df=pd.read_csv(test_path, sep='\\t')\n",
        "\n",
        "train_path = '/content/gdrive/MyDrive/Advanced-DataAnalytics-Assignment2/train.csv'\n",
        "\n",
        "train_df=pd.read_csv(train_path, sep='\\t')\n",
        "\n",
        "train_df['image-path'] = train_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "test_df['image-path'] = test_df.apply(lambda row: str(row['imageid']) + \".jpg\", axis=1)\n",
        "\n",
        "train_df = train_df.astype({'imageid':'string'})\n",
        "test_df = test_df.astype({'imageid':'string'})\n",
        "\n",
        "cat_cols = ['label']\n",
        "\n",
        "# apply label encoder to categorical columns\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(train_df['label'])\n",
        "train_df[cat_cols] = train_df[cat_cols].apply(lambda x: le.fit_transform(x))\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "train_df['label'].unique()\n",
        "unique_labels = np.unique(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DDtLwGmFDk_"
      },
      "source": [
        "Here we are doing image augmentation. Image augmentation is a technique commonly used in deep learning to artificially increase the amount of data available for training a model. By applying various transformations to existing images, such as flipping, rotating, scaling, or changing their color scheme, we can generate new images that are similar but not identical to the original ones. This can help the model to learn more robust features and generalize better to unseen data.\n",
        "\n",
        "Two popular data augmentation tools for image classification tasks are horizontal flip and grayscale conversion. Horizontal flip flips the image along the vertical axis, creating a mirror image that still contains the same object but from a different perspective. This can help the model to recognize objects regardless of their orientation in the input image. Grayscale conversion, on the other hand, converts the original RGB image into a single-channel grayscale image, removing color information while preserving the overall shape and texture of the object. This can help the model to focus on the most relevant features of the image, such as edges and contours, and ignore less important details such as color variations.\n",
        "\n",
        "So, here we are using horizontal flip and grayscale conversion to increase the data-points via image augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eca45ufC2j_j"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "# selecting random images from the train.csv\n",
        "\n",
        "# taking 30 percent of random images from the train.csv to horizontally flip\n",
        "\n",
        "random_1 = train_df.sample(frac = 0.3)\n",
        "counter = 60001\n",
        "for ind in random_1.index:\n",
        "  img = Image.open('Fashion-Product-Images/images/' + random_1['image-path'][ind])\n",
        "  horizontal_flip = transforms.RandomHorizontalFlip(p = 1)\n",
        "  hflipped = horizontal_flip(img)\n",
        "  path = 'Fashion-Product-Images/images/' + str(counter) + '.jpg'\n",
        "  hflipped.save(path)\n",
        "  image_path = str(counter) + '.jpg'\n",
        "  train_df = train_df.append({'imageid' : counter, 'label' : random_1['label'][ind], 'productname' : random_1['productname'][ind], 'image-path' : image_path }, ignore_index=True )\n",
        "  counter+=1\n",
        "\n",
        "# taking 30 percent of random images from the train.csv to do grayscale\n",
        "\n",
        "random_2 = train_df.sample(frac = 0.3)\n",
        "\n",
        "for ind in random_2.index:\n",
        "  img = Image.open('Fashion-Product-Images/images/' + random_2['image-path'][ind])\n",
        "  grayscale = transforms.Grayscale()\n",
        "  gscale = grayscale(img)\n",
        "  path = 'Fashion-Product-Images/images/' + str(counter) + '.jpg'\n",
        "  gscale.save(path)\n",
        "  image_path = str(counter) + '.jpg'\n",
        "  train_df = train_df.append({'imageid' : counter, 'label' : random_2['label'][ind], 'productname' : random_2['productname'][ind], 'image-path' : image_path }, ignore_index=True )\n",
        "  counter+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhUNR_4k8f52"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(train_df, shuffle = True, test_size = 0.3)\n",
        "\n",
        "# basic setup for PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "\n",
        "  \n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "\n",
        "val_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(), # changing the datatype to tensor because pytorch takes input as tensor\n",
        "\n",
        "])\n",
        "\n",
        "image_path = 'Fashion-Product-Images/images/'\n",
        "\n",
        "class CustomImageDataset_from_csv(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        print(img_path)\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label)\n",
        "\n",
        "class CustomImageDataset_from_csv_test(Dataset):\n",
        "    def __init__(self, dataframe , img_dir ,  transform = None , label_transform = None):\n",
        "        self.img_labels = dataframe #pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    \n",
        "    def __getitem__(self , idx):\n",
        "        img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 3])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        if self.label_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return(image, label, img_path)\n",
        "\n",
        "class FirstCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstCNN, self).__init__()\n",
        "       \n",
        "        # input size [3, 128, 128]\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "            \n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 13)\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqKIEdFoFNAY"
      },
      "outputs": [],
      "source": [
        "_exp_name = \"sample_4\"\n",
        "batch_size = 256\n",
        "\n",
        "train_data = CustomImageDataset_from_csv(train_df , image_path , transform = train_tfm)\n",
        "train_dataloader = DataLoader(train_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "val_data = CustomImageDataset_from_csv(val_df , image_path , transform = val_tfm)\n",
        "val_dataloader = DataLoader(val_data, batch_size = batch_size , shuffle = True, num_workers=0, pin_memory=True)\n",
        "\n",
        "test_data = CustomImageDataset_from_csv_test(test_df , image_path , transform = test_tfm)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size , shuffle = False, num_workers=0, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUKJVAqYFS_R"
      },
      "outputs": [],
      "source": [
        "# Selecting which device to use for training. It would be better to use cuda if GPU is available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initializing the model, and put it on the device specified.\n",
        "model = FirstCNN().to(device)\n",
        "\n",
        "# Initialize optimizer!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5) \n",
        "\n",
        "# For the classification task, we use cross-entropy as the measurement of performance.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# The number of training epochs (hyperparameters) and patience.\n",
        "n_epochs = 4\n",
        "patience = 300 # If no improvement in 'patience' epochs, early stop\n",
        "\n",
        "\n",
        "# Initializing trackers\n",
        "stale = 0\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    ## Evaluation and training on training dataset.\n",
        "\n",
        "    model.train()        # model is in train mode before training.\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.to(device)  # move images to device\n",
        "        labels = torch.tensor(labels).to(device)  # move labels to device\n",
        "\n",
        "        # Forward the data\n",
        "        logits = model(imgs)\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the gradients for parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "        \n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    ## Evaluation and testing on validation dataset\n",
        "\n",
        "    model.eval()       # model is in eval mode\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # Iterate the validation set by batches.\n",
        "    for batch in tqdm(val_dataloader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "        #break\n",
        "\n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc:\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CekjaR8iFXBq"
      },
      "outputs": [],
      "source": [
        "model_best = FirstCNN().to(device)\n",
        "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
        "model_best.eval()\n",
        "\n",
        "prediction = []\n",
        "image_paths = []\n",
        "\n",
        "## Encodings\n",
        "# {0: Bags, 1: Bottomwear, 2: Eyewear, 3: Fragrance, 4: Innerwear, 5: Jewellery, 6: Makeup, 7: Others, 8: Sandal, 9: Shoes, 10: Topwear, 11: Wallets, 12: watches}\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data,_, img in test_dataloader:\n",
        "        test_pred = model_best(data.to(device))\n",
        "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
        "        prediction += test_label.squeeze().tolist()\n",
        "        image_paths += list(img)\n",
        "\n",
        "\n",
        "print(prediction)\n",
        "print(image_paths)\n",
        "mapping = {0: \"Bags\", 1: \"Bottomwear\", 2: \"Eyewear\", 3: \"Fragrance\", 4: \"Innerwear\", 5: \"Jewellery\", 6: \"Makeup\", 7: \"Others\", 8: \"Sandal\", 9: \"Shoes\", 10: \"Topwear\", 11: \"Wallets\", 12: \"watches\" }\n",
        "\n",
        "# now creating a prediction csv\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(prediction)):\n",
        "  # append rows to an empty DataFrame\n",
        "  df = df.append({'image-path' : image_paths[i], 'encoded-label' : prediction[i], 'label-decoded' : mapping[prediction[i]]},\n",
        "        ignore_index = True)\n",
        "\n",
        "df.to_csv(\"prediction_4.csv\",index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4dQ6sVwm37c"
      },
      "source": [
        "## Conclusion:\n",
        "\n",
        "In conclusion, image augmentation using horizontal flip and grayscale conversion has been shown to be an effective technique for increasing the amount of data available for our training of deep learning model. By generating new images with similar but not identical features to the original ones, we improved the generalization ability of the model and prevented overfitting to the training dataset. \n",
        "\n",
        "Moreover, by artificially increasing the amount of data using various image transformations, we can provide the model with a diverse set of examples to learn from, making it more robust to variations in the input images.\n",
        "\n",
        "The use of horizontal flip and grayscale conversion in particular has allowed us to increase the number of data-points in our dataset, leading to better performance and accuracy of the model. Horizontal flip creates a mirror image of the input image by flipping it along the vertical axis. This helps the model to learn object features that are invariant to horizontal orientation, improving its ability to recognize objects regardless of their orientation in the input image. Grayscale conversion, on the other hand, removes color information from the input image, making the model more focused on the most relevant features of the image, such as edges and contours.\n",
        "\n",
        "Lastly, our model performed better than the previous models where no image augmentation was done. This is because the additional training data provides the model with more examples to learn from, leading to improved generalization ability and reduced overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOEeCOw9FYMi"
      },
      "source": [
        "## Thank You! Keep Smiling and Stay Happy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
